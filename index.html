<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Wanhua Li</title>
  
  <meta name="author" content="Wanhua Li">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Wanhua Li</name>
              </p>
              <p>I am currently a postdoctoral fellow at <a href="https://www.harvard.edu/">Harvard University</a> supervised by Prof. <a href="https://vcg.seas.harvard.edu/people">Hanspeter Pfister</a>. 
		 Prior to that, I received my Ph.D. from the Department of Automation at <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a> in 2022, advised by Prof. <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>, 
		  Prof. <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/">Jianjiang Feng </a>, and Prof. <a href="https://www.au.tsinghua.edu.cn/info/1084/1699.htm">Jie Zhou</a>. 
		      In 2017, I received my B.S. degree in computer science at <a href="https://www.sysu.edu.cn/sysuen/">Sun Yat-sen University</a>, Guangzhou, China. 
		</p>
		    
		    <p>My research interests mainly include face analysis, neural rendering, and 3D-aware synthesis. </p>
			  
		    <p> <b>If you are interested in my research or would like to work with me as an intern at Harvard University, feel free to contact me. Remote collaboration is also welcome!</b>
		    </p>
		    
		    <p> Email: wanhua [AT] seas [DOT] harvard [DOT] edu  <br /> <!-- <del> li-wh17 [AT] mails [DOT] tsinghua [DOT] edu [DOT] cn </del>  --> </p>
              <p style="text-align:center">
                <!-- <a href="mailto:wanhua016@gmail.com"> Email </a> &nbsp/&nbsp -->
                <a href="data/CV_Wanhua.pdf"> CV </a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=I03QnrsAAAAJ"> Google Scholar </a> &nbsp/&nbsp
		<a href="https://twitter.com/Wanhua_Ethan_Li">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/Li-Wanhua"> GitHub </a> 
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/wanhua.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/wanhua_m_circle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
		
		
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
               <li style="margin: 5px;">
                <b>2024-02:</b> Two papers on 3D Gaussian splatting and multi-task learning are accepted by <a href="https://cvpr.thecvf.com/">CVPR 2024</a>.
              </li>  
	     <li style="margin: 5px;">
                <b>2023-08:</b> One paper on talking head synthesis is accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046">TMM</a>.
              </li>     
	      <li style="margin: 5px;">
                <b>2023-07:</b> One paper on face clustering is accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">T-PAMI</a>.
              </li> 
	       <li style="margin: 5px;">
                <b>2023-07:</b> Two papers on face clustering and foundation models are accepted by <a href="https://iccv2023.thecvf.com/">ICCV 2023</a>.
              </li>        
	     <li style="margin: 5px;">
                <b>2023-06:</b> One paper on deepfake detection is accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">TIP</a>.
              </li>    
	     <li style="margin: 5px;">
                <b>2023-02:</b>  One paper on talking head synthesis is accepted to <a href="https://cvpr.thecvf.com/">CVPR 2023</a>.
              </li>
	     <li style="margin: 5px;">
                <b>2022-10:</b> I joined Harvard as a postdoc!
              </li>
		<li style="margin: 5px;">
                <b>2022-09:</b> One paper on language-guided ordinal regression is accepted  by <a href="https://neurips.cc/Conferences/2022">NeurIPS 2022</a>.
              </li>
	       <li style="margin: 5px;">
                <b>2022-07:</b> Two papers on multi-attribute learning and talking head synthesis are accepted  by <a href="https://eccv2022.ecva.net/">ECCV 2022</a>.
              </li>
	      <li style="margin: 5px;">
                <b>2022-06:</b> One paper on age estimation is accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">TIP</a>.
              </li>
	      <li style="margin: 5px;">
                <b>2022-04:</b> One paper on image inpainting is accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046">TMM</a>.
              </li>
	      <li style="margin: 5px;">
                <b>2021-10:</b> Our team won the 3rd place in <a href="https://vipriors.github.io/challenges/">2021 VIPriors Instance Segmentation Challenge</a> (ICCV 2021).
              </li>		    
	      <li style="margin: 5px;">
                <b>2021-07:</b> One paper on video inpainting detection is accepted by <a href="http://iccv2021.thecvf.com/home">ICCV 2021</a>.
              </li>
               <li style="margin: 5px;">
                <b>2021-04:</b> One paper on  kinship verification is accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">TIP</a>.
              </li>
	      <li style="margin: 5px;">
                <b>2021-03:</b> Three papers on uncertainty learning, kinship verification, and face clustering are accepted to <a href="http://cvpr2021.thecvf.com/">CVPR 2021</a>.
              </li>
		   <!--
              <li style="margin: 5px;" >
                <b>2020-07:</b> One paper on social relation recognition is accepted to <a href="https://eccv2020.eu/">ECCV 2020</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2020-03:</b> One paper on kinship verification is accepted as oral presentation at <a href="https://www.2020.ieeeicme.org/"> ICME 2020</a>.
              </li>
              <li style="margin: 5px;">
                <b>2019-02:</b> One paper on age estimation is accepted to <a href="http://cvpr2019.thecvf.com/">CVPR 2019</a>.
              </li>
                -->
            </p>
          </td>
        </tr>
      </tbody></table>
	  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
			  <p></p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

       <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/langsplat.jpg' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>LangSplat: 3D Language Gaussian Splatting</papertitle>
              <br>
		Minghan Qin*, <strong>Wanhua Li*†</strong>, Jiawei Zhou1*, Haoqian Wang†, Hanspeter Pfister	 
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024
			  <br>
              <a href="https://langsplat.github.io/">[Website]</a> 
	      <a href="https://arxiv.org/abs/2312.16084">[arxiv]</a> 
	      <a href="https://www.youtube.com/watch?v=XMlyjsei-Es&ab_channel=minghanqin">[Video]</a> 
              <a href="https://github.com/minghanqin/LangSplat">[Code]</a>
              <br>
              <p> We ground CLIP features into a set of 3D language Gaussians, which attains precise 3D language fields while being 199 × faster than LERF.</p>
            </td>
          </tr>
		
	<tr>	 
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/training.gif' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>CLIPTrans: Transferring Visual Knowledge with Pre-trained Models for Multimodal Machine Translation</papertitle>
              <br>
			Devaansh Gupta, Siddhant Kharbanda, Jiawei Zhou, <strong>Wanhua Li</strong>, Hanspeter Pfister, and Donglai Wei
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2023

			  <br>
              <a href="https://devaansh100.github.io/projects/cliptrans/">[Website]</a> 
	      <a href="https://arxiv.org/abs/2308.15226">[arxiv]</a> 
              <a href="https://github.com/devaansh100/CLIPTrans">[Code]</a>
	      <a href="https://www.youtube.com/watch?v=Mhapx61G9Zk">[Video]</a>
              <br>
              <p>To facilitate using pre-trained models in MMT, we propose CLIPTrans, which transfers the multimodal representations of M-CLIP into a multilingual mBART.</p>
            </td>
         </tr>	
	
	<tr>	 
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/CLIP-Cluster.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>CLIP-Cluster: CLIP-Guided Attribute Hallucination for Face Clustering</papertitle>
              <br>
			Shuai Shen, <strong>Wanhua Li</strong>, Xiaobing Wang, Dafeng Zhang, Zhezhu Jin, Jie Zhou, and Jiwen Lu 
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2023

			  <br>
              <a href="">[Website]</a> 
	      <a href="">[arxiv]</a> 
              <a href="">[Code]</a>
	      <a href="">[Video]</a>
              <br>
              <p>We propose an attribute hallucination framework named CLIP-Cluster to narrow the intraclass variance caused by different face attributes for face clustering.</p>
            </td>
         </tr>	

	 <tr>	 
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/OrdinalCLIP.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>OrdinalCLIP: Learning Rank Prompts for Language-Guided Ordinal Regression</papertitle>
              <br>
			  <strong>Wanhua Li*</strong>, Xiaoke Huang*, Zheng Zhu, Yansong Tang, Xiu Li, Jie Zhou, and Jiwen Lu

              <br>
              <em>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2022
			  <br>
              <a href="https://xk-huang.github.io/OrdinalCLIP/">[Website]</a> 
	      <a href="https://arxiv.org/abs/2206.02338">[arxiv]</a> 
              <a href="https://github.com/xk-huang/OrdinalCLIP">[Code]</a>
		    <a href="https://zhuanlan.zhihu.com/p/565034693">[中文解读]</a>
              <br>
              <p>We propose a language-powered paradigm for ordinal regression, which learns the rank concepts from the rich semantic CLIP latent space.</p>
            </td>
         </tr>
		
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/l2l.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Label2Label: A Language Modeling Framework for Multi-Attribute Learning</papertitle>
              <br>
			  <strong>Wanhua Li</strong>, Zhexuan Cao, Jianjiang Feng, Jie Zhou, and Jiwen Lu
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2022
			  <br>
              <a href="https://li-wanhua.github.io/Label2Label/">[Website]</a> 
	      <a href="https://arxiv.org/pdf/2207.08677.pdf">[arxiv]</a> 
	      <a href="https://www.youtube.com/watch?v=999znKklDb4">[Video]</a> 
              <a href="https://github.com/Li-Wanhua/Label2Label">[Code]</a>
              <br>
              <p> We propose a language modeling framework named Label2Label to model the complex instance-wise attribute relations, 
		      which regards each attribute label as a “word” and recovers the label “sentence” based on the masked one.</p>
            </td>
         </tr>
		

		
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/talkinghead2.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Dynamic Facial Radiance Fields for Few-Shot Talking Head Synthesis</papertitle>
              <br>
			Shuai Shen, <strong>Wanhua Li</strong>, Zheng Zhu, Yueqi Duan, Jie Zhou, and Jiwen Lu
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2022
			  <br>
              <a href="https://sstzal.github.io/DFRF/">[Website]</a> 
	      <a href="https://arxiv.org/abs/2207.11770">[arxiv]</a> 
	      <a href="https://www.youtube.com/watch?v=F6fkVNk9bBw&ab_channel=Shens">[Video]</a> 
              <a href="https://github.com/sstzal/DFRF">[Code]</a>
              <br>
              <p> We propose dynamic facial radiance fields conditioned on the 3D aware reference image features. 
		      The facial field can rapidly generalize to novel identities with only 15s clip.</p>
            </td>
         </tr>
		
<!-- 	<tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/MetaAge.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>MetaAge: Meta-Learning Personalized Age Estimators</papertitle>
              <br>
			 <strong>Wanhua Li</strong>, Jiwen Lu, Abudukelimu Wuerkaixi, Jianjiang Feng, and Jie Zhou
              <br>
              <em>IEEE Transactions on Image Processing</em>, 2022
			  <br>
	      <a href="https://li-wanhua.github.io/MetaAge/">[Website]</a> 
              <a href="https://ieeexplore.ieee.org/document/9826407">[Paper]</a> 
	      <a href="http://arxiv.org/abs/2207.05288">[arxiv]</a> 
	      <a href="https://github.com/Li-Wanhua/MetaAge">[Code]</a> 
              <br>
              <p> We propose a personalized age estimation method named MetaAge, which learns the mapping from identity information to age estimator parameters.</p>
            </td>
          </tr>  -->
		
        
<!-- 	<tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/MRIN.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Adaptive Patch Generators for Mask-Robust Image Inpainting</papertitle>
              <br>
			  Hongyi Sun, <strong>Wanhua Li</strong>, Yueqi Duan, Jie Zhou, and Jiwen Lu
              <br>
              <em>IEEE Transactions on Multimedia</em>, 2022
			  <br>
	      <a href="https://ieeexplore.ieee.org/document/9773024">[Paper]</a>
	      <a href="data/tmm22.txt">[bibtex]</a> 
              <br>
              <p> We propose a Mask-Robust Inpainting Network (MRIN) to recover the masked areas of an image with a patch-wise inpainting process.</p>
            </td>
          </tr> -->
		
		
<!-- 	<tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/HRGN.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Reasoning Graph Networks for Kinship Verification: from Star-shaped to Hierarchical</papertitle>
              <br>
			  <strong>Wanhua Li</strong>, Jiwen Lu, Abudukelimu Wuerkaixi, Jianjiang Feng, and Jie Zhou
              <br>
              <em>IEEE Transactions on Image Processing</em>, 2021
			  <br>
	      <a href="https://ieeexplore.ieee.org/document/9426411">[Paper]</a>
	      <a href="https://arxiv.org/pdf/2109.02219.pdf">[arxiv]</a>
	      <a href="data/tip20_rgn.txt">[bibtex]</a> 
              <br>
              <p> We develop a Hierarchical Reasoning Graph Network (H-RGN) to exploit more powerful and flexible capacity for graph-based kinship verification.</p>
            </td>
          </tr> -->
       
		
	<tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/FAST.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Frequency-Aware Spatiotemporal Transformers for Video Inpainting Detection</papertitle>
              <br>
			  Bingyao Yu, <strong>Wanhua Li</strong>, Xiu Li, Jiwen Lu, and Jie Zhou
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
			  <br>
              
	      <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Yu_Frequency-Aware_Spatiotemporal_Transformers_for_Video_Inpainting_Detection_ICCV_2021_paper.pdf" >[Paper]</a> 
	      <a href="data/iccv21.txt">[bibtex]</a> 
              <br>
              <p> We propose a Frequency-Aware Spatiotemporal Transformer for video inpainting detection, which simultaneously mines the traces of video inpainting from spatial, temporal, and frequency domains.</p>
            </td>
          </tr>
		
       <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/uncertainty_regression.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Probabilistic Ordinal Embeddings for Uncertainty-Aware Regression</papertitle>
              <br>
			  <strong>Wanhua Li</strong>, Xiaoke Huang, Jiwen Lu, Jianjiang Feng, and Jie Zhou
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2021
			  <br>
              <a href="https://li-wanhua.github.io/POEs/">[Website]</a> 
	      <a href="https://arxiv.org/abs/2103.13629">[arxiv]</a> 
	      <a href="https://www.youtube.com/watch?v=NCJZyvKZ8vc">[Video]</a> 
              <a href="https://github.com/Li-Wanhua/POEs">[Code]</a>
              <br>
              <p> We propose probabilistic ordinal embeddings to empower the present-day regression methods with the ability of uncertainty estimation.</p>
            </td>
          </tr>
		
      <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/DSMM.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Meta-Mining Discriminative Samples for Kinship Verification</papertitle>
              <br>
			  <strong>Wanhua Li</strong>, Shiwei Wang, Jiwen Lu, Jianjiang Feng, and Jie Zhou
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2021
			  <br>
	      <a href="https://li-wanhua.github.io/DSMM/">[Website]</a> 
              <a href="https://arxiv.org/abs/2103.15108">[arxiv]</a> 
	      <a href="https://www.youtube.com/watch?v=gEQETp4hCAc">[Video]</a> 
              <a href="data/CVPR2021meta.txt">[bibtex]</a>
              <br>
              <p> A Discriminative Sample Meta-Mining strategy is proposed to mine discriminative information from limited positive pairs and sufficient negative samples for kinship verification. </p>
            </td>
          </tr>
		
	<tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/STAR_FC.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Structure-Aware Face Clustering on a Large-Scale Graph with 10^7 Nodes</papertitle>
              <br>
			  Shuai Shen, <strong>Wanhua Li</strong>, Zheng Zhu, Guan Huang, Dalong Du, Jiwen Lu, and Jie Zhou
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2021
			  <br>
              <a href="https://sstzal.github.io/STAR-FC/">[Website]</a> 
              <a href="https://arxiv.org/abs/2103.13225">[arxiv]</a> 
              <a href="https://github.com/sstzal/STAR-FC">[Code]</a>
	      <a href="https://www.youtube.com/watch?v=VwAomM3wk6k">[Video]</a> 
              <br>
              <p> It is the first face clustering method to train on very large-scale graph with 20M nodes, and achieve superior inference results on 12M testing data.</p>
            </td>
          </tr>
		
		  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/social_graph.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Graph-Based Social Relation Reasoning</papertitle>
              <br>
			  <strong>Wanhua Li</strong>, Yueqi Duan, Jiwen Lu, Jianjiang Feng, and Jie Zhou
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2020
			  <br>
	      <a href="https://li-wanhua.github.io/GR2N/">[Website]</a> 
              <a href="https://arxiv.org/abs/2007.07453">[arxiv]</a> 
	      <a href="https://www.youtube.com/watch?v=zCTPRxxlZsI&t=427s">[Video]</a> 
              <a href="https://github.com/Li-Wanhua/GR2N">[Code]</a>
              <br>
              <p> A simpler, faster, and more accurate method for social relation recognition.</p>
            </td>
          </tr>
	
		<!--
		    <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/graph_kinship.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Graph-based Kinship Reasoning Network</papertitle>
              <br>
			  <strong>Wanhua Li</strong>, Yingqiang Zhang, Kangchen Lv, Jiwen Lu, Jianjiang Feng, Jie Zhou
              <br>
              <em>IEEE International Conference on Multimedia and Expo (ICME)</em>, 2020
			  <br>
			  <font color="red"><strong>Oral Presentation</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2004.10375">[arXiv]</a>  
			  <a href="https://www.youtube.com/watch?v=QkMiKE30Lv4">[Video]</a> 
			  <a href="data/icme20_graphkinship.txt">[bibtex]</a> 
              <br>
              <p> We considers how to compare and fuse the extracted feature pair to reason about the kin relations with the proposed graph-based kinship reasoning networks. </p>
            </td>
          </tr>
	-->	  
		    <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/bridge.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>BridgeNet: A Continuity-Aware Probabilistic Network for Age Estimation</papertitle>
              <br>
			  <strong>Wanhua Li</strong>, Jiwen Lu, Jianjiang Feng, Chunjing Xu, Jie Zhou, Qi Tian
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2019
              <br>
              <a href="https://arxiv.org/abs/1904.03358">[arXiv]</a>  
			  <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_BridgeNet_A_Continuity-Aware_Probabilistic_Network_for_Age_Estimation_CVPR_2019_paper.pdf">[PDF]</a>  
			  <a href="data/cvpr19_BridgeNet.txt">[bibtex]</a> 
              <br>
              <p> We propose BridgeNet for age estimation, which aims to mine the continuous relation between age labels effectively. </p>
            </td>
          </tr>

		  

        </tbody></table>
		
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Honors and Awards</heading>
            <p>
		     <li style="margin: 5px;" >
                NeurIPS Scholar Award, 2022.
              </li>
		           <li style="margin: 5px;" >
                ICCV Doctoral Consortium Travel Award, 2021.
              </li>
		           <li style="margin: 5px;" >
                Weihai Talent Scholarship, Tsinghua, 2021.
              </li>
			  <li style="margin: 5px;" >
                3rd Place in 2021 VIPriors Instance Segmentation Challenge @ICCV 2021.
              </li>
		   <li style="margin: 5px;" >
                Outstanding Oral Presentation at Beijing University Academic Forum on Artificial Intelligence, 2021
              </li>
			  <li style="margin: 5px;" >
                2nd Place in ChaLearn LAP Large-scale Isolated Gesture Recognition Challenge @ICCV 2017.
              </li>
		          <li style="margin: 5px;" >
                Outstanding Undergraduate Thesis, SYSU, 2017.
              </li>
			  <li style="margin: 5px;" >
                Outstanding Graduate, SYSU, 2017.
              </li>
		          <li style="margin: 5px;" >
                National Encouragement Scholarship, Ministry of Education of P.R. China, 2016.
              </li>
		          <li style="margin: 5px;" >
                National Scholarship, Ministry of Education of P.R. China, 2015.
              </li>
			  <li style="margin: 5px;" >
                National Scholarship, Ministry of Education of P.R. China, 2014.
              </li>

            </p>
          </td>
        </tr>
      </tbody></table>
		
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Professional Activities</heading>
            <p> 
		    <li style="margin: 5px;" >
                <b>Reviewer, </b> IEEE Transactions on Pattern Analysis and Machine Intelligence.
              </li>   
	      <li style="margin: 5px;" >
                <b>Reviewer, </b> IEEE Transactions on Image Processing.
              </li>
	      <li style="margin: 5px;" >
                <b>Reviewer, </b> IEEE Transactions on Neural Networks and Learning Systems.
              </li>
	      <li style="margin: 5px;" >
                <b>Reviewer, </b> IEEE Transactions on Circuits and Systems for Video Technology.
              </li>
	      <li style="margin: 5px;" >
                <b>Reviewer, </b> IEEE Transactions on Biometrics, Behavior, and Identity Science.
              </li>	
	      <li style="margin: 5px;" >
                <b>Reviewer, </b> IEEE Transactions on Artificial Intelligence.
              </li>
	      <li style="margin: 5px;" >
                <b>Reviewer, </b> IEEE Transactions on Cybernetics.
              </li> 
	      <li style="margin: 5px;" >
                <b>Reviewer, </b> IEEE Signal Processing Letters.
              </li>
	      <li style="margin: 5px;" >
                <b>Reviewer, </b> International Journal of Computer Vision.
              </li>     
	      <li style="margin: 5px;" >
                <b>Reviewer, </b> Pattern Recognition.
              </li>
		 <li style="margin: 5px;" >
                <b>Reviewer, </b> Neural Networks.
              </li>
		<li style="margin: 5px;" >
                <b>Reviewer, </b> Neurocomputing.
              </li>	
              <li style="margin: 5px;" >
                <b>Reviewer, </b> Pattern Recognition Letters.
              </li>
		<li style="margin: 5px;" >
                <b>Reviewer, </b> Journal of Visual Communication and Image Representation.
              </li>
	      <li style="margin: 5px;" >
                <b>Reviewer, </b> Knowledge-Based Systems.
              </li>		    
	      <li style="margin: 5px;" >
                <b>Reviewer, </b> Frontiers of Computer Science.
              </li>
              <li style="margin: 5px;" >
                <b>Reviewer,</b> International Conference on Computer Vision (ICCV), 2021-2023.
              </li>
	      <li style="margin: 5px;" >
                <b>Reviewer,</b> IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022-2023.
              </li>
	      <li style="margin: 5px;" >
                <b>Reviewer,</b> European Conference on Computer Vision (ECCV), 2022.
              </li>
	      <li style="margin: 5px;" >
                <b>Reviewer,</b> Conference on Neural Information Processing Systems (NeurIPS), 2023.
              </li>		    
	      <li style="margin: 5px;" >
                <b>PC member,</b> AAAI Conference on Artificial Intelligence (AAAI), 2022-2024.
              </li>
	      <li style="margin: 5px;" >
                <b>PC member,</b> International Joint Conference on Artificial Intelligence (IJCAI), 2022-2023.
              </li>
              <li style="margin: 5px;" >
                <b>Reviewer,</b> IEEE International Conference on Multimedia and Expo (ICME), 2019-2023.
              </li>
              <li style="margin: 5px;">
                <b>Reviewer,</b> IEEE International Conference on Image Processing (ICIP), 2018-2023.
              </li>
	      <li style="margin: 5px;">
                <b>Reviewer,</b> International Conference on Pattern Recognition (ICPR), 2018-2022.
              </li>
	      <li style="margin: 5px;">
                <b>Reviewer,</b> Chinese Conference on Pattern Recognition and Computer Vision (PRCV), 2021-2023.
              </li>
	      <li style="margin: 5px;">
                <b>Reviewer,</b> IEEE International Conference on Automatic Face and Gesture Recognition (FG), 2023-2024.
              </li>		    
            </p>
          </td>
        </tr>
      </tbody></table>
	  
	  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
<a href="https://clustrmaps.com/site/1bek5" title="Visit tracker"><img src="//clustrmaps.com/map_v2.png?cl=ffffff&w=a&t=tt&d=-ATGl6RdulzjfzWwVcyEb_4b7GcVGNxQTy08mLud4cQ" /></a>
	          
       
      </td>
    </tr>
  </table>
</body>

</html>
